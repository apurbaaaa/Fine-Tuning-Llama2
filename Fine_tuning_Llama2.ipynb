{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOvHMkNnDLSpDIAF5g5d2k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apurbaaaa/Fine-Tuning-Llama2/blob/main/Fine_tuning_Llama2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8WufdjvACvkj"
      },
      "outputs": [],
      "source": [
        "!pip install -q accelerate peft bitsandbytes transformers trl sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer"
      ],
      "metadata": {
        "id": "KtXiZ3fSDBF9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
        "\n",
        " dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
        "\n",
        " new_model = \"Llama-2-7b-chat-finetune\""
      ],
      "metadata": {
        "id": "8UbOJJbWAv-p"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#QLoRA params required\n",
        "\n",
        "#attention dimension, Rank\n",
        "lora_r = 64\n",
        "\n",
        "#alpha parameter for scaling\n",
        "lora_alpha = 16\n",
        "\n",
        "#Drop\n",
        "lora_dropout = 0.1\n",
        "\n",
        "#for quantization, bitsnandbytes used\n",
        "use_4bit = True\n",
        "#Aactivate 4 bit precision base models\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "\n",
        "#Quantization type (fp4 or nf4)\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "\n",
        "#Activate nested quantization for 4-bit base models (double quantization)\n",
        "use_nested_quant = False"
      ],
      "metadata": {
        "id": "5eVZK0igHDFG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = \"./results\"\n",
        "\n",
        "#number of training epochs\n",
        "num_train_epochs = 1\n",
        "\n",
        "#Enable fp16/bf16 training (set bf16 to True with an A100)\n",
        "fp16 = False\n",
        "bf16 = False\n",
        "\n",
        "#Batch size for training\n",
        "per_device_train_batch_size = 4\n",
        "\n",
        "#Batch size for eval\n",
        "per_device_eval_batch_size = 4\n",
        "\n",
        "#number of steps to accumulate gradients for\n",
        "gradient_accumulation_steps = 1\n",
        "\n",
        "#Enable gradient checkpointing\n",
        "gradient_checkpointing = True\n",
        "\n",
        "#Maximum gradient normal (gradient clipping)\n",
        "max_grad_norm = 0.3\n",
        "\n",
        "learning_rate = 2e-4\n",
        "\n",
        "#Weight decay to apply to all laters except bias/LayerNorm weights\n",
        "weight_decay = 0.001\n",
        "\n",
        "#optimizer\n",
        "optim = \"paged_adamw_32bit\"\n",
        "\n",
        "# Learning rate schedule\n",
        "lr_schedule_type = \"cosine\"\n",
        "\n",
        "#Number of training steps (overrides num_train_epochs)\n",
        "max_steps = -1\n",
        "\n",
        "#Ratio of steps for a linear warmup\n",
        "warmup_ratio = 0.03"
      ],
      "metadata": {
        "id": "FY8GmI3OISCF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Group sequences into batch with same lenght\n",
        "#saves memory and speeds up training\n",
        "\n",
        "group_by_length = True\n",
        "\n",
        "# Save checkpoint every X updates steps\n",
        "save_steps = 0\n",
        "\n",
        "# Log every X uupdates steps\n",
        "logging_steps = 25"
      ],
      "metadata": {
        "id": "vzrjNisGJok6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SFT Params\n",
        "max_seq_length = None\n",
        "\n",
        "#pack multiple short exmples in the same input sequence to increase efficiency\n",
        "packing = False\n",
        "\n",
        "# Load model on GPU 0\n",
        "devcice_map = {\"\": 0}"
      ],
      "metadata": {
        "id": "3Wi6pXVjJ83D"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\n",
        "    \"mlabonne/guanaco-llama2-1k\",\n",
        "    split=\"train\",\n",
        "    streaming=True\n",
        ")\n",
        "\n",
        "#loading tokenizer and model with QLoRA config\n",
        "compute_dtpe = getattr(torch, bnb_4bit_compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type = bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype = bnb_4bit_compute_dtype,\n",
        "    bnb_4bit_use_double_quant = use_nested_quant,\n",
        ")\n",
        "\n",
        "#Check GPU compatibilty with bfloat16\n",
        "\n",
        "if compute_dtype == torch.float16 and use_4bit:\n",
        "    major, _ = torch.cuda.get_device_capability()\n",
        "    if major >= 8:\n",
        "        print(\"=\" * 80)\n",
        "        print(\"Your GPU supports bfloat16: accelerate training with bf16=True\")\n",
        "        print(\"=\" * 80)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config = bnb_config,\n",
        "    device_map = device_map\n",
        ")\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1\n",
        "\n",
        "#load llama tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code = True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "#load lora config\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha = lora_alpha,\n",
        "    lora_dropout = lora_dropout,\n",
        "    r = lora_r,\n",
        "    bias = \"none\",\n",
        "    task_type = \"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "#Set training parameters\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    num_train_epochs = num_train_epochs,\n",
        "    per_device_train_batch_size = per_device_train_batch_size,\n",
        "    gradient_accumulation_steps = gradient_accumulation_steps,\n",
        "    optim = optim,\n",
        "    save_steps = save_steps,\n",
        "    logging_steps = logging_steps,\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay = weight_decay,\n",
        "    fp16 = fp16,\n",
        "    bf16 = bf16,\n",
        "    max_grad_norm = max_grad_norm,\n",
        "    max_steps = max_steps,\n",
        "    warmup_ratio = warmup_ratio,\n",
        "    group_by_length = group_by_length,\n",
        "    lr_scheduler_type = lr_scheduler_type,\n",
        "    report_to = \"tensorboard\"\n",
        ")\n",
        "\n",
        "#set supervised fine-tuning parameters\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    peft_config = peft_config,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    tokenizer = tokenizer,\n",
        "    args = training_arguments,\n",
        "    packing = packing,\n",
        ")\n",
        "\n",
        "#Train model\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "sZLqjJu4KP2d",
        "outputId": "bbf5e9f6-88be-46f4-e961-a59c81371f87"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "Loading a streaming dataset cached in a LocalFileSystem is not supported yet.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a9b8bb6dccf9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m dataset = load_dataset(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"mlabonne/guanaco-llama2-1k\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mstreaming\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   2127\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mrequested\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mIf\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdataset_path\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mof\u001b[0m \u001b[0ma\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0mdict\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0meach\u001b[0m \u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2129\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2130\u001b[0m     \u001b[0mExample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/builder.py\u001b[0m in \u001b[0;36mas_streaming_dataset\u001b[0;34m(self, split, base_path)\u001b[0m\n\u001b[1;32m   1316\u001b[0m         \u001b[0mExample\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1318\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m   1319\u001b[0m                     datasets.SplitGenerator(\n\u001b[1;32m   1320\u001b[0m                             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Loading a streaming dataset cached in a LocalFileSystem is not supported yet."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pOrydacuMMmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}